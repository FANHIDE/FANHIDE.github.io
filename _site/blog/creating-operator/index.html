<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Yunfan Zhang" />
    <meta name="description" content="Personal website and resume">
    <link rel="shortcut icon" type="image/x-icon" href="https://fanhide.github.io/img/favicon.ico">
    <title>创建 Kubernetes Operator Demo</title>
    <meta name="generator" content="Hugo 0.55.6" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fanhide.github.io/css/main.css" /><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/tomorrow.min.css">
    
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">

      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="https://fanhide.github.io/"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="/blog/">BLOG</a></li>
        
        <li><a href="/projects/">PROJECTS</a></li>
        
        <li><a href="/resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      
      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="https://fanhide.github.io/blog/creating-operator/">创建 Kubernetes Operator Demo</a></strong>
          </h3>
        </div>
        <div class="blog-title">
          <h4>
          August 29, 2019
            &nbsp;&nbsp;
            
          </h4>
        </div>
        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              

<h3 id="introduction"><strong>Introduction</strong></h3>

<p>在 Kubernetes 的监控方案中我们经常会使用到一个Promethues Operator的项目，该项目可以让我们更加方便的去使用 Prometheus，而不需要直接去使用最原始的一些资源对象，比如 Pod、Deployment，随着 Prometheus Operator 项目的成功，CoreOS 公司开源了一个比较厉害的工具：Operator Framework，该工具可以让开发人员更加容易的开发 Operator 应用。</p>

<p>在本篇文章中我们会为大家介绍一个简单示例来演示如何使用 Operator Framework 框架来开发一个 Operator 应用。</p>

<h3 id="workflow"><strong>Workflow</strong></h3>

<p>Operator SDK 提供以下工作流来开发一个新的 Operator：</p>

<ol>
<li>使用 SDK 创建一个新的 Operator 项目</li>
<li>通过添加自定义资源（CRD）定义新的资源 API</li>
<li>指定使用 SDK API 来 watch 的资源</li>
<li>定义 Operator 的协调（reconcile）逻辑</li>
<li>使用 Operator SDK 构建并生成 Operator 部署清单文件
<br /></li>
</ol>

<h3 id="demo"><strong>Demo</strong></h3>

<p>我们平时在部署一个简单的 Webserver 到 Kubernetes 集群中的时候，都需要先编写一个 Deployment 的控制器，然后创建一个 Service 对象，通过 Pod 的 label 标签进行关联，最后通过 Ingress 或者 type=NodePort 类型的 Service 来暴露服务，每次都需要这样操作，是不是略显麻烦，我们就可以创建一个自定义的资源对象，通过我们的 CRD 来描述我们要部署的应用信息，比如镜像、服务端口、环境变量等等，然后创建我们的自定义类型的资源对象的时候，通过控制器去创建对应的 Deployment 和 Service，是不是就方便很多了，相当于我们用一个资源清单去描述了 Deployment 和 Service 要做的两件事情。</p>

<p>这里我们将创建一个名为 AppService 的 CRD 资源对象，然后定义如下的资源清单进行应用部署：</p>

<pre><code>apiVersion: app.example.com/v1
kind: AppService
metadata:
  name: nginx-app
spec:
  size: 2
  image: nginx:1.7.9
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30002
</code></pre>

<blockquote>
<p>通过这里的自定义的 AppService 资源对象去创建副本数为2的 Pod，然后通过 nodePort=30002 的端口去暴露服务，接下来我们就来一步一步的实现我们这里的这个简单的 Operator 应用。</p>
</blockquote>

<h3 id="开发环境">开发环境</h3>

<h5 id="环境需求">环境需求</h5>

<p>要开发 Operator 自然 Kubernetes 集群是少不了的，还需要 Golang 的环境，这里的安装就不多说了，然后还需要一个 Go 语言的依赖管理工具包：GO111MODULE，是go 1.11版本之后自带的，需要在环境变量中设置GO111MODULE=on</p>

<h5 id="安装-operator-sdk">安装 operator-sdk</h5>

<p>operator sdk 安装方法非常多，我们可以直接在 github 上面下载需要使用的版本，然后放置到 PATH 环境下面即可，当然也可以将源码 clone 到本地手动编译安装即可，如果你是 Mac，当然还可以使用常用的 brew 工具进行安装：</p>

<pre><code>$ brew install operator-sdk
......
$ operator-sdk version
operator-sdk version: v0.7.0
$ go version
go version go1.11.4 darwin/amd64
</code></pre>

<h3 id="演示">演示</h3>

<h5 id="创建新项目">创建新项目</h5>

<p>环境准备好了，接下来就可以使用 operator-sdk 直接创建一个新的项目了，命令格式为： operator-sdk new
按照上面我们预先定义的 CRD 资源清单，我们这里可以这样创建：
未科学上网的话，我们知道从 Go 1.11 版本开始，新增了 GOPROXY 环境变量。如果设置了该变量，下载源代码时将会通过这个环境变量设置的代理地址，而不再是以前的直接从代码库下载。同时，也提供了公用的代理服务 <a href="https://goproxy.io，我们只需设置该环境变量即可正常下载被墙的源码包了：">https://goproxy.io，我们只需设置该环境变量即可正常下载被墙的源码包了：</a></p>

<pre><code>export GOPROXY=https://goproxy.io
$ cd $GOPATH/src/github.com/hide
$ operator-sdk new opdemo
......
......
$ $ tree -L 2
.
├── build
│   ├── Dockerfile
│   └── bin
├── cmd
│   └── manager
├── deploy
│   ├── operator.yaml
│   ├── role.yaml
│   ├── role_binding.yaml
│   └── service_account.yaml
├── go.mod
├── go.sum
├── pkg
│   ├── apis
│   └── controller
├── tools.go
└── version
    └── version.go


9 directories, 9 files	
</code></pre>

<p>到这里一个全新的 Operator 项目就新建完成了。</p>

<h5 id="项目结构">项目结构</h5>

<p>使用operator-sdk new命令创建新的 Operator 项目后，项目目录就包含了很多生成的文件夹和文件。</p>

<ol>
<li>go.mod —  GOMODULE清单，用来描述当前 Operator 的依赖包。</li>
<li>cmd - 包含 main.go 文件，使用 operator-sdk API 初始化和启动当前 Operator 的入口。</li>
<li>deploy - 包含一组用于在 Kubernetes 集群上进行部署的通用的 Kubernetes 资源清单文件。</li>
<li>pkg/apis - 包含定义的 API 和自定义资源（CRD）的目录树，这些文件允许 sdk 为 CRD 生成代码并注册对应的类型，以便正确解码自定义资源对象。</li>
<li>pkg/controller - 用于编写所有的操作业务逻辑的地方
我们主要需要编写的是pkg目录下面的 api 定义以及对应的 controller 实现。</li>
</ol>

<h5 id="添加api">添加API</h5>

<p>接下来为我们的自定义资源添加一个新的 API，按照上面我们预定义的资源清单文件，在 Operator 相关根目录下面执行如下命令：</p>

<pre><code>$ operator-sdk add api --api-version=app.example.com/v1 --kind=AppService
</code></pre>

<h5 id="添加控制器">添加控制器</h5>

<p>上面我们添加自定义的 API，接下来可以添加对应的自定义 API 的具体实现 Controller，同样在项目根目录下面执行如下命令：</p>

<pre><code>$ operator-sdk add controller --api-version=app.example.com/v1 --kind=AppService
</code></pre>

<p>这样整个 Operator 项目的脚手架就已经搭建完成了，接下来就是具体的实现了。</p>

<h5 id="自定义-api">自定义 API</h5>

<p>打开源文件pkg/apis/app/v1/appservice_types.go，需要我们根据我们的需求去自定义结构体 AppServiceSpec，我们最上面预定义的资源清单中就有 size、image、ports 这些属性，所有我们需要用到的属性都需要在这个结构体中进行定义：</p>

<pre><code>type AppServiceSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run &quot;operator-sdk generate k8s&quot; to regenerate code after modifying this file
	// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html
	Size  	  *int32                      `json:&quot;size&quot;`
	Image     string                      `json:&quot;image&quot;`
	Resources corev1.ResourceRequirements `json:&quot;resources,omitempty&quot;`
	Envs      []corev1.EnvVar             `json:&quot;envs,omitempty&quot;`
	Ports     []corev1.ServicePort        `json:&quot;ports,omitempty&quot;`
}
</code></pre>

<p>代码中会涉及到一些包名的导入，由于包名较多，所以我们会使用一些别名进行区分，主要的包含下面几个：</p>

<pre><code>import (
    appsv1 &quot;k8s.io/api/apps/v1&quot;
    corev1 &quot;k8s.io/api/core/v1&quot;
    appv1 &quot;github.com/hide/opdemo/pkg/apis/app/v1&quot;
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)
</code></pre>

<p>这里的 resources、envs、ports 的定义都是直接引用的&rdquo;k8s.io/api/core/v1&rdquo;中定义的结构体，而且需要注意的是我们这里使用的是ServicePort，而不是像传统的 Pod 中定义的 ContanerPort，这是因为我们的资源清单中不仅要描述容器的 Port，还要描述 Service 的 Port。</p>

<p>然后一个比较重要的结构体AppServiceStatus用来描述资源的状态，当然我们可以根据需要去自定义状态的描述，我这里就偷懒直接使用 Deployment 的状态了：</p>

<pre><code>type AppServiceStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run &quot;operator-sdk generate k8s&quot; to regenerate code after modifying this file
	// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html
	appsv1.DeploymentStatus `json:&quot;,inline&quot;`
}
</code></pre>

<p>定义完成后，在项目根目录下面执行如下命令：</p>

<pre><code>$ operator-sdk generate k8s
</code></pre>

<p>该命令是用来根据我们自定义的 API 描述来自动生成一些代码，目录pkg/apis/app/v1/下面以zz_generated开头的文件就是自动生成的代码，里面的内容并不需要我们去手动编写。</p>

<p>这样我们就算完成了对自定义资源对象的 API 的声明。</p>

<h5 id="实现业务逻辑">实现业务逻辑</h5>

<p>上面 API 描述声明完成了，接下来就需要我们来进行具体的业务逻辑实现了，编写具体的 controller 实现，打开源文件pkg/controller/appservice/appservice_controller.go，需要我们去更改的地方也不是很多，核心的就是Reconcile方法，该方法就是去不断的 watch 资源的状态，然后根据状态的不同去实现各种操作逻辑，核心代码如下：</p>

<pre><code>func (r *ReconcileAppService) Reconcile(request reconcile.Request) (reconcile.Result, error) {
	reqLogger := log.WithValues(&quot;Request.Namespace&quot;, request.Namespace, &quot;Request.Name&quot;, request.Name)
	reqLogger.Info(&quot;Reconciling AppService&quot;)

	// Fetch the AppService instance
	instance := &amp;appv1.AppService{}
	err := r.client.Get(context.TODO(), request.NamespacedName, instance)
	if err != nil {
		if errors.IsNotFound(err) {
			// Request object not found, could have been deleted after reconcile request.
			// Owned objects are automatically garbage collected. For additional cleanup logic use finalizers.
			// Return and don't requeue
			return reconcile.Result{}, nil
		}
		// Error reading the object - requeue the request.
		return reconcile.Result{}, err
	}

	if instance.DeletionTimestamp != nil {
		return reconcile.Result{}, err
	}

	// 如果不存在，则创建关联资源
	// 如果存在，判断是否需要更新
	//   如果需要更新，则直接更新
	//   如果不需要更新，则正常返回

	deploy := &amp;appsv1.Deployment{}
	if err := r.client.Get(context.TODO(), request.NamespacedName, deploy); err != nil &amp;&amp; errors.IsNotFound(err) {
		// 创建关联资源
		// 1. 创建 Deploy
		deploy := resources.NewDeploy(instance)
		if err := r.client.Create(context.TODO(), deploy); err != nil {
			return reconcile.Result{}, err
		}
		// 2. 创建 Service
		service := resources.NewService(instance)
		if err := r.client.Create(context.TODO(), service); err != nil {
			return reconcile.Result{}, err
		}
		// 3. 关联 Annotations
		data, _ := json.Marshal(instance.Spec)
		if instance.Annotations != nil {
			instance.Annotations[&quot;spec&quot;] = string(data)
		} else {
			instance.Annotations = map[string]string{&quot;spec&quot;: string(data)}
		}

		if err := r.client.Update(context.TODO(), instance); err != nil {
			return reconcile.Result{}, nil
		}
		return reconcile.Result{}, nil
	}

	oldspec := appv1.AppServiceSpec{}
	if err := json.Unmarshal([]byte(instance.Annotations[&quot;spec&quot;]), oldspec); err != nil {
		return reconcile.Result{}, err
	}

	if !reflect.DeepEqual(instance.Spec, oldspec) {
		// 更新关联资源
		newDeploy := resources.NewDeploy(instance)
		oldDeploy := &amp;appsv1.Deployment{}
		if err := r.client.Get(context.TODO(), request.NamespacedName, oldDeploy); err != nil {
			return reconcile.Result{}, err
		}
		oldDeploy.Spec = newDeploy.Spec
		if err := r.client.Update(context.TODO(), oldDeploy); err != nil {
			return reconcile.Result{}, err
		}

		newService := resources.NewService(instance)
		oldService := &amp;corev1.Service{}
		if err := r.client.Get(context.TODO(), request.NamespacedName, oldService); err != nil {
			return reconcile.Result{}, err
		}
		oldService.Spec = newService.Spec
		if err := r.client.Update(context.TODO(), oldService); err != nil {
			return reconcile.Result{}, err
		}

		return reconcile.Result{}, nil
	}

	return reconcile.Result{}, nil

}
</code></pre>

<p>上面就是业务逻辑实现的核心代码，逻辑很简单，就是去判断资源是否存在，不存在，则直接创建新的资源，创建新的资源除了需要创建 Deployment 资源外，还需要创建 Service 资源对象，因为这就是我们的需求，当然你还可以自己去扩展，比如在创建一个 Ingress 对象。更新也是一样的，去对比新旧对象的声明是否一致，不一致则需要更新，同样的，两种资源都需要更新的。</p>

<p>另外两个核心的方法就是上面的resources.NewDeploy(instance)和resources.NewService(instance)方法，这两个方法实现逻辑也很简单，就是根据 CRD 中的声明去填充 Deployment 和 Service 资源对象的 Spec 对象即可。</p>

<p>NewDeploy 方法实现如下：</p>

<pre><code>func NewDeploy(app *appv1.AppService) *appsv1.Deployment {
	labels := map[string]string{&quot;app&quot;: app.Name}
	selector := &amp;metav1.LabelSelector{MatchLabels: labels}
	return &amp;appsv1.Deployment{
		TypeMeta: metav1.TypeMeta{
			APIVersion: &quot;apps/v1&quot;,
			Kind:       &quot;Deployment&quot;,
		},
		ObjectMeta: metav1.ObjectMeta{
			Name:      app.Name,
			Namespace: app.Namespace,

			OwnerReferences: []metav1.OwnerReference{
				*metav1.NewControllerRef(app, schema.GroupVersionKind{
					Group: v1.SchemeGroupVersion.Group,
					Version: v1.SchemeGroupVersion.Version,
					Kind: &quot;AppService&quot;,
				}),
			},
		},
		Spec: appsv1.DeploymentSpec{
			Replicas: app.Spec.Size,
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels: labels,
				},
				Spec: corev1.PodSpec{
					Containers: newContainers(app),
				},
			},
			Selector: selector,
		},
	}
}

func newContainers(app *v1.AppService) []corev1.Container {
	containerPorts := []corev1.ContainerPort{}
	for _, svcPort := range app.Spec.Ports {
		cport := corev1.ContainerPort{}
		cport.ContainerPort = svcPort.TargetPort.IntVal
		containerPorts = append(containerPorts, cport)
	}
	return []corev1.Container{
		{
			Name: app.Name,
			Image: app.Spec.Image,
			Resources: app.Spec.Resources,
			Ports: containerPorts,
			ImagePullPolicy: corev1.PullIfNotPresent,
			Env: app.Spec.Envs,
		},
	}
}
</code></pre>

<p>newService 对应的方法实现如下：</p>

<pre><code>unc NewService(app *v1.AppService) *corev1.Service {
	return &amp;corev1.Service {
		TypeMeta: metav1.TypeMeta {
			Kind: &quot;Service&quot;,
			APIVersion: &quot;v1&quot;,
		},
		ObjectMeta: metav1.ObjectMeta{
			Name: app.Name,
			Namespace: app.Namespace,
			OwnerReferences: []metav1.OwnerReference{
				*metav1.NewControllerRef(app, schema.GroupVersionKind{
					Group: v1.SchemeGroupVersion.Group,
					Version: v1.SchemeGroupVersion.Version,
					Kind: &quot;AppService&quot;,
				}),
			},
		},
		Spec: corev1.ServiceSpec{
			Type: corev1.ServiceTypeNodePort,
			Ports: app.Spec.Ports,
			Selector: map[string]string{
				&quot;app&quot;: app.Name,
			},
		},
	}
}
</code></pre>

<p>这样我们就实现了 AppService 这种资源对象的业务逻辑。</p>

<h5 id="调试">调试</h5>

<p>如果我们本地有一个可以访问的 Kubernetes 集群，我们也可以直接进行调试，在本地用户~/.kube/config文件中配置集群访问信息，下面的信息表明可以访问 Kubernetes 集群：</p>

<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://ydzs-master:6443
KubeDNS is running at https://ydzs-master:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>

<p>首先，在集群中安装 CRD 对象：</p>

<pre><code>$ kubectl create -f deploy/crds/app_v1_appservice_crd.yaml
customresourcedefinition &quot;appservices.app.example.com&quot; created
$ kubectl get crd
NAME                                   AGE
appservices.app.example.com            &lt;invalid&gt;
......
</code></pre>

<p>当我们通过kubectl get crd命令获取到我们定义的 CRD 资源对象，就证明我们定义的 CRD 安装成功了。其实现在只是 CRD 的这个声明安装成功了，但是我们这个 CRD 的具体业务逻辑实现方式还在我们本地，并没有部署到集群之中，我们可以通过下面的命令来在本地项目中启动 Operator 的调试：</p>

<pre><code>$ operator-sdk up local                                                     
INFO[0000] Running the operator locally.                
INFO[0000] Using namespace default.                     
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.964137,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Go Version: go1.11.4&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.964192,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Go OS/Arch: darwin/amd64&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.9641972,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Version of operator-sdk: v0.7.0&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.965905,&quot;logger&quot;:&quot;leader&quot;,&quot;msg&quot;:&quot;Trying to become the leader.&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.965945,&quot;logger&quot;:&quot;leader&quot;,&quot;msg&quot;:&quot;Skipping leader election; not running in a cluster.&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.928867,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Registering Components.&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.929077,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting EventSource&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;source&quot;:&quot;kind source: /, Kind=&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.9292521,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting EventSource&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;source&quot;:&quot;kind source: /, Kind=&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.622659,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;failed to initialize service object for metrics: OPERATOR_NAME must be set&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.622693,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Starting the Cmd.&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.7236018,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting Controller&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.8284118,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting workers&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;worker count&quot;:1}
</code></pre>

<p>上面的命令会在本地运行 Operator 应用，通过~/.kube/config去关联集群信息，现在我们去添加一个 AppService 类型的资源然后观察本地 Operator 的变化情况，资源清单文件就是我们上面预定义的（deploy/crds/app_v1_appservice_cr.yaml）</p>

<pre><code>apiVersion: app.example.com/v1
kind: AppService
metadata:
  name: nginx-app
spec:
  size: 2
  image: nginx:1.7.9
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30002
</code></pre>

<p>直接创建这个资源对象：</p>

<pre><code>$ kubectl create -f deploy/crds/app_v1_appservice_cr.yaml
appservice &quot;nginx-app&quot; created
</code></pre>

<p>我们可以看到我们的应用创建成功了，这个时候查看 Operator 的调试窗口会有如下的信息出现：</p>

<pre><code>......
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207416.670523,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207417.004226,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207417.004331,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207418.33779,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207418.951193,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;}
......
</code></pre>

<p>然后我们可以去查看集群中是否有符合我们预期的资源出现：</p>

<pre><code>$ kubectl get AppService
NAME        AGE
nginx-app   &lt;invalid&gt;
$ kubectl get deploy
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-app                2         2         2            2           &lt;invalid&gt;
$ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        76d
nginx-app    NodePort    10.108.227.5   &lt;none&gt;        80:30002/TCP   &lt;invalid&gt;
$ kubectl get pods
NAME                                      READY     STATUS    RESTARTS   AGE
nginx-app-76b6449498-2j82j                1/1       Running   0          &lt;invalid&gt;
nginx-app-76b6449498-m4h58                1/1       Running   0          &lt;invalid&gt;
</code></pre>

<p>我们定义了两个副本（size=2），这里就出现了两个 Pod，还有一个 NodePort=30002 的 Service 对象</p>

<h5 id="部署">部署</h5>

<p>自定义的资源对象现在测试通过了，但是如果我们将本地的operator-sdk up local命令终止掉，我们可以猜想到就没办法处理 AppService 资源对象的一些操作了，所以我们需要将我们的业务逻辑实现部署到集群中去。</p>

<p>执行下面的命令构建 Operator 应用打包成 Docker 镜像：</p>

<pre><code>$ operator-sdk build hide/opdemo                         
INFO[0002] Building Docker image hide/opdemo           
Sending build context to Docker daemon  400.7MB
Step 1/7 : FROM registry.access.redhat.com/ubi7-dev-preview/ubi-minimal:7.6
......
Successfully built a8cde91be6ab
Successfully tagged hide/opdemo:latest
INFO[0053] Operator build complete.              
</code></pre>

<p>镜像构建成功后，推送到 docker hub：
$ docker push hide/opdemo</p>

<p>镜像推送成功后，使用上面的镜像地址更新 Operator 的资源清单：</p>

<pre><code>$ sed -i 's|REPLACE_IMAGE|hide/opdemo|g' deploy/operator.yaml
# 如果你使用的是 Mac 系统，使用下面的命令
$ sed -i &quot;&quot; 's|REPLACE_IMAGE|hide/opdemo|g' deploy/operator.yaml
</code></pre>

<p>现在 Operator 的资源清单文件准备好了，然后创建对应的 RBAC 的对象：</p>

<pre><code># Setup Service Account
$ kubectl create -f deploy/service_account.yaml
# Setup RBAC
$ kubectl create -f deploy/role.yaml
$ kubectl create -f deploy/role_binding.yaml
</code></pre>

<p>权限相关声明已经完成，接下来安装 CRD 和 Operator：</p>

<pre><code># Setup the CRD
$ kubectl apply -f deploy/crds/app_v1_appservice_crd.yaml
$ kubectl get crd
NAME                                   CREATED AT
appservices.app.example.com            2019-05-30T17:03:32Z
......
# Deploy the Operator
$ kubectl create -f deploy/operator.yaml
deployment.apps/opdemo created
$ kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
opdemo-64db96d575-9vtq6                   1/1     Running   0          2m2s
</code></pre>

<p>到这里我们的 CRD 和 Operator 实现都已经安装成功了。</p>

<p>现在我们再来部署我们的 AppService 资源清单文件，现在的业务逻辑就会在上面的opdemo-64db96d575-9vtq6的 Pod 中去处理了。</p>

<pre><code>$ kubectl create -f deploy/crds/app_v1_appservice_cr.yaml
appservice.app.example.com/nginx-app created
$ kubectl get appservice
NAME        AGE
nginx-app   18s
$  kubectl get deploy
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
nginx-app                2/2     2            2           24s
opdemo                   1/1     1            1           5m51s
$  kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        76d
nginx-app    NodePort    10.106.129.82   &lt;none&gt;        80:30002/TCP   29s
opdemo       ClusterIP   10.100.233.51   &lt;none&gt;        8383/TCP       4m25s
$  kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
nginx-app-76b6449498-ffhgx                1/1     Running   0          32s
nginx-app-76b6449498-wzjq2                1/1     Running   0          32s
opdemo-64db96d575-9vtq6                   1/1     Running   0          5m59s
$ kubectl describe appservice nginx-app
Name:         nginx-app
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  spec: {&quot;size&quot;:2,&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;resources&quot;:{},&quot;ports&quot;:[{&quot;protocol&quot;:&quot;TCP&quot;,&quot;port&quot;:80,&quot;targetPort&quot;:80,&quot;nodePort&quot;:30002}]}
API Version:  app.example.com/v1
Kind:         AppService
Metadata:
  Creation Timestamp:  2019-05-30T17:41:28Z
  Generation:          2
  Resource Version:    19666617
  Self Link:           /apis/app.example.com/v1/namespaces/default/appservices/nginx-app
  UID:                 2756f232-8302-11e9-80ca-525400cc3c00
Spec:
  Image:  nginx:1.7.9
  Ports:
    Node Port:    30002
    Port:         80
    Protocol:     TCP
    Target Port:  80
  Resources:
  Size:  2
Events:  &lt;none&gt;
</code></pre>

<p>然后同样的可以通过 30002 这个 NodePort 端口去访问应用，到这里应用就部署成功了。</p>

              <hr>
              <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div>
            </div>
          </div>
          <hr>
        <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
        </div>
      </div>
      
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="https://fanhide.github.io/js/docs.min.js"></script>
<script src="https://fanhide.github.io/js/main.js"></script>

<script src="https://fanhide.github.io/js/ie10-viewport-bug-workaround.js"></script><!-- Syntax highlighting -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>
